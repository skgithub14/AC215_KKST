{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EDA.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Oea1DUMA_bvY"},"source":["# Project EDA and Data Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"ctkGlJ9lAZmy"},"source":["## Set-up"]},{"cell_type":"code","metadata":{"id":"9ayvnStyActh","executionInfo":{"status":"ok","timestamp":1635220982864,"user_tz":240,"elapsed":317,"user":{"displayName":"Edward Bayes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBS8uPqNfCqU1vJVNp318bH_svd_rbG4HtV7-5mQ=s64","userId":"09014545777139056358"}}},"source":["import os\n","from os.path import join  \n","import requests\n","import zipfile\n","import tarfile\n","import shutil\n","import random\n","import math\n","import json\n","import time\n","import sys\n","import cv2\n","import string\n","import re\n","import subprocess\n","import hashlib\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import collections\n","import unicodedata\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","%matplotlib inline\n","\n","# Tensorflow\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.utils.layer_utils import count_params\n","\n","# sklearn\n","from sklearn.model_selection import train_test_split\n","\n","# Tensorflow Hub\n","import tensorflow_hub as hub\n","\n","# Colab auth\n","from google.colab import auth\n","from google.cloud import storage"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4kzvxwbRH7cf"},"source":["### GitHub Integration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9Q9ZAOa_Syx","executionInfo":{"status":"ok","timestamp":1635220985124,"user_tz":240,"elapsed":1913,"user":{"displayName":"Edward Bayes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBS8uPqNfCqU1vJVNp318bH_svd_rbG4HtV7-5mQ=s64","userId":"09014545777139056358"}},"outputId":"97546d33-bda8-4f7e-c84b-31664c6ed7e2"},"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT, force_remount=True)           # we mount the google drive at /content/drive"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYgvir8NBFDK","executionInfo":{"status":"ok","timestamp":1635220985451,"user_tz":240,"elapsed":333,"user":{"displayName":"Edward Bayes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBS8uPqNfCqU1vJVNp318bH_svd_rbG4HtV7-5mQ=s64","userId":"09014545777139056358"}},"outputId":"6b649178-741d-45d2-91b1-e4847d6e2df7"},"source":["# Integrate GitHub\n","\n","# each group member should add their own ENV_PATH here\n","# comment out other people's ENV_PATHs\n","# Steve:\n","# ENV_PATH = \"/content/drive/MyDrive/adv_practical_data_science/local-repo/AC215_KKST/.env\"\n","# Matt:\n","# Shih-ye:\n","# Al:\n","# Ed:\n","ENV_PATH = \"/content/drive/MyDrive/AC215_ED/AC215_KKST/.env\"\n","\n","# load environment variables\n","with open(ENV_PATH) as env:\n","  env_text = env.read()\n","env_list = env_text.split(\"\\n\")\n","PROJECT_PATH = env_list[0]\n","GIT_PATH = env_list[1]\n","EMAIL = env_list[2]\n","GIT_USERNAME = env_list[3]\n","\n","# expand paths\n","REPO_PATH = PROJECT_PATH + '/AC215_KKST'\n","NOTEBOOK_DIR_PATH = REPO_PATH + '/notebooks'\n","DATA_PATH = REPO_PATH + '/data'\n","\n","# change directory to the local repo's notebook folder\n","%cd '{NOTEBOOK_DIR_PATH}'"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AC215_ED/AC215_KKST/notebooks\n"]}]},{"cell_type":"markdown","metadata":{"id":"5GD5ZrLuIEd7"},"source":["#### Cells for updating local/remote repos"]},{"cell_type":"code","metadata":{"id":"Md8eyXoIIW1Z"},"source":["# This will update your local repo with work other group member's have done on the project so you can build on their work.\n","!git pull"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4Wdt2IRHEH6"},"source":["# check statuses of the files you changed\n","# this will give a list of files currently in the \"head\"\n","!git status"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmdUCtQiIZrA"},"source":["# add files you changed to the head\n","# this is where you tell Git which files you changed that you want to update your local repo with\n","\n","# add all files in your local repo to the head\n","!git add .\n","\n","# add all files you changed to the head\n","#!git add -u\n","\n","# add files by name that you want to add to the head\n","#!git add {filename1}\n","\n","# check statuses of the files you changed\n","# this will give a list of files currently in the \"head\"\n","!git status"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pAcgf0E6IfP2"},"source":["# commit the changes in the head to your local repo\n","# note if your message is too long the the commit will not work, keep it under 50 characters.\n","!git commit -m \"updated git add\"\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{GIT_USERNAME}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IhWojgFJzSt"},"source":["# push the changes in your local repo to the group's remote repo develop branch\n","#!git push origin develop\n","\n","# push the changes in your local repo to the group's remote repo master branch\n","# note that only changes which have been thorough tested and validated should be committed to the master branch\n","!git push origin\n","\n","# you can also create additional branches for the remote repo as desired"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kbeh3FY8ReWi"},"source":["### Verify Set-up"]},{"cell_type":"code","metadata":{"id":"Yje1bCrBRvBF"},"source":["# Enable/Disable Eager Execution\n","# Reference: https://www.tensorflow.org/guide/eager\n","# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n","# without building graphs\n","\n","#tf.compat.v1.disable_eager_execution()\n","#tf.compat.v1.enable_eager_execution()\n","\n","print(\"tensorflow version\", tf.__version__)\n","print(\"keras version\", tf.keras.__version__)\n","print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n","\n","# Get the number of replicas \n","strategy = tf.distribute.MirroredStrategy()\n","print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n","\n","devices = tf.config.experimental.get_visible_devices()\n","print(\"Devices:\", devices)\n","print(tf.config.experimental.list_logical_devices('GPU'))\n","\n","print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n","print(\"All Physical Devices\", tf.config.list_physical_devices())\n","\n","# Better performance with the tf.data API\n","# Reference: https://www.tensorflow.org/guide/data_performance\n","AUTOTUNE = tf.data.experimental.AUTOTUNE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzevKCiJR0hz"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yy7FOHrBSPI0"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"zFEWcQ0dSM7g"},"source":["def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n","  if base_path != \"\":\n","    if not os.path.exists(base_path):\n","      os.mkdir(base_path)\n","  packet_file = os.path.basename(packet_url)\n","  with requests.get(packet_url, stream=True, headers=headers) as r:\n","      r.raise_for_status()\n","      with open(os.path.join(base_path,packet_file), 'wb') as f:\n","          for chunk in r.iter_content(chunk_size=8192):\n","              f.write(chunk)\n","  \n","  if extract:\n","    if packet_file.endswith(\".zip\"):\n","      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n","        zfile.extractall(base_path)\n","    else:\n","      packet_name = packet_file.split('.')[0]\n","      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n","        tfile.extractall(base_path)\n","\n","def compute_dataset_metrics(data_list):\n","  data_list_with_metrics = []\n","  for item in data_list:\n","    img_path = FIMAGES + '/' + item[0]\n","    image = cv2.imread(img_path)\n","    data_list_with_metrics.append((img_path,image.shape[0],image.shape[1],image.nbytes / (1024 * 1024.0)))\n","\n","  # Build a dataframe\n","  data_list_with_metrics = np.asarray(data_list_with_metrics)\n","  dataset_df = pd.DataFrame({\n","    'path': data_list_with_metrics[:, 0],\n","    'height': data_list_with_metrics[:, 1],\n","    'width': data_list_with_metrics[:, 2],\n","    'size': data_list_with_metrics[:, 3],\n","    })\n","  \n","  dataset_df[\"height\"] = dataset_df[\"height\"].astype(int)\n","  dataset_df[\"width\"] = dataset_df[\"width\"].astype(int)\n","  dataset_df[\"size\"] = dataset_df[\"size\"].astype(float)\n","\n","  dataset_mem_size = dataset_df[\"size\"].sum()\n","  height_details = dataset_df[\"height\"].describe()\n","  width_details = dataset_df[\"width\"].describe()\n","\n","  print(\"Dataset Metrics:\")\n","  print(\"----------------\")\n","  print(\"Image Width:\")\n","  print(\"Min:\",width_details[\"min\"],\" Max:\",width_details[\"max\"])\n","  print(\"Image Height:\")\n","  print(\"Min:\",height_details[\"min\"],\" Max:\",height_details[\"max\"])\n","  print(\"Size in memory:\",round(dataset_df[\"size\"].sum(),2),\"MB\")\n","\n","class JsonEncoder(json.JSONEncoder):\n","  def default(self, obj):\n","    if isinstance(obj, np.integer):\n","        return int(obj)\n","    elif isinstance(obj, np.floating):\n","        return float(obj)\n","    elif isinstance(obj, decimal.Decimal):\n","        return float(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    else:\n","        return super(JsonEncoder, self).default(obj)\n","\n","experiment_name = None\n","def create_experiment():\n","  global experiment_name\n","  experiment_name = \"experiment_\" + str(int(time.time()))\n","\n","  # Create experiment folder\n","  if not os.path.exists(experiment_name):\n","      os.mkdir(experiment_name)\n","\n","def upload_experiment(data_details):\n","  # Check Bucket Access\n","  bucket_name = \"ac215-mushroom-app-models\" # BUCKET NAME\n","\n","  # List buckets in a GCP project\n","  storage_client = storage.Client(project=\"ac215-project\") # PROJECT ID \n","\n","  # Get bucket for Experiments\n","  bucket = storage_client.get_bucket(bucket_name)\n","  print(\"Model Bucket:\",bucket)\n","\n","  save_data_details(data_details)\n","\n","  # Copy the experiment folder to GCP Bucket\n","  for file_path in glob(experiment_name+'/*'):\n","    print(file_path)\n","    blob = bucket.blob(os.path.join(user_account,file_path)) \n","    print('uploading file', file_path)\n","    blob.upload_from_filename(file_path)\n","\n","def save_data_details(data_details):\n","  with open(os.path.join(experiment_name,\"data_details.json\"), \"w\") as json_file:\n","    json_file.write(json.dumps(data_details,cls=JsonEncoder))\n","\n","def save_model(model,model_name=\"model01\"):\n","\n","  # Save the enitire model (structure + weights)\n","  model.save(os.path.join(experiment_name,model_name+\".hdf5\"))\n","\n","  # Save only the weights\n","  model.save_weights(os.path.join(experiment_name,model_name+\".h5\"))\n","\n","  # Save the structure only\n","  model_json = model.to_json()\n","  with open(os.path.join(experiment_name,model_name+\".json\"), \"w\") as json_file:\n","      json_file.write(model_json)\n","\n","def get_model_size(model_name=\"model01\"):\n","  model_size = os.stat(os.path.join(experiment_name,model_name+\".h5\")).st_size\n","  return model_size\n","\n","def append_training_history(model_train_history, prev_model_train_history):\n","  for metric in [\"loss\",\"val_loss\",\"accuracy\",\"val_accuracy\"]:\n","    for metric_value in prev_model_train_history[metric]:\n","      model_train_history[metric].append(metric_value)\n","  \n","  return model_train_history\n","\n","def evaluate_save_model(model,test_data, model_train_history,execution_time, learning_rate, batch_size, epochs, optimizer,save=True):\n","  \n","  # Get the number of epochs the training was run for\n","  num_epochs = len(model_train_history[\"loss\"])\n","\n","  # Plot training results\n","  fig = plt.figure(figsize=(15,5))\n","  axs = fig.add_subplot(1,2,1)\n","  axs.set_title('Loss')\n","  # Plot all metrics\n","  for metric in [\"loss\",\"val_loss\"]:\n","      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n","  axs.legend()\n","  \n","  axs = fig.add_subplot(1,2,2)\n","  axs.set_title('Accuracy')\n","  # Plot all metrics\n","  for metric in [\"accuracy\",\"val_accuracy\"]:\n","      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n","  axs.legend()\n","\n","  plt.show()\n","  \n","  # Evaluate on test data\n","  evaluation_results = model.evaluate(test_data)\n","  print(evaluation_results)\n","  \n","  if save:\n","    # Save model\n","    save_model(model, model_name=model.name)\n","    model_size = get_model_size(model_name=model.name)\n","\n","    # Save model history\n","    with open(os.path.join(experiment_name,model.name+\"_train_history.json\"), \"w\") as json_file:\n","        json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n","\n","    trainable_parameters = count_params(model.trainable_weights)\n","    non_trainable_parameters = count_params(model.non_trainable_weights)\n","\n","    # Save model metrics\n","    metrics ={\n","        \"trainable_parameters\":trainable_parameters,\n","        \"execution_time\":execution_time,\n","        \"loss\":evaluation_results[0],\n","        \"accuracy\":evaluation_results[1],\n","        \"model_size\":model_size,\n","        \"learning_rate\":learning_rate,\n","        \"batch_size\":batch_size,\n","        \"epochs\":epochs,\n","        \"optimizer\":type(optimizer).__name__\n","    }\n","    with open(os.path.join(experiment_name,model.name+\"_model_metrics.json\"), \"w\") as json_file:\n","        json_file.write(json.dumps(metrics,cls=JsonEncoder))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHK1UBxBRhIK"},"source":["## Data Sets"]},{"cell_type":"markdown","metadata":{"id":"JkEBNKNLRyiE"},"source":["### Download and Read In"]},{"cell_type":"code","metadata":{"id":"3DXyq8E5SaVh"},"source":["# FLICKR data\n","# download\n","start_time = time.time()\n","download_file(\"https://storage.googleapis.com/ac215-project/flickr_data.zip\", base_path=DATA_PATH + '/flickr', extract=True)\n","execution_time = (time.time() - start_time)/60.0\n","print(\"Download execution time (mins)\",execution_time)\n","\n","# read-in\n","FLICKR_PATH = DATA_PATH + '/flickr'\n","fcaps = pd.read_csv(os.path.join(FLICKR_PATH,\"captions.txt\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7T6-fT4RR0QU"},"source":["### Explore Data Shape"]},{"cell_type":"code","metadata":{"id":"lp0bBFcg9mo_"},"source":["# explore the captions\n","print(\"Number of rows:\",fcaps.shape[0])\n","print(\"Unique image names:\",len(pd.unique(fcaps['image'])))\n","print(\"Unique captions:\",len(pd.unique(fcaps['caption'])))\n","print(\"Unique rows:\",len(fcaps.value_counts()))\n","print(\"Head:\")\n","fcaps.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvVrmQsaDiDa"},"source":["# explore the images\n","FCAPTIONS = FLICKR_PATH + '/Images'\n","print(\"Counting all .txt files in: \" + FCAPTIONS)\n","x=0\n","for files in os.listdir(FCAPTIONS):\n","    if files.endswith('.txt'):\n","        x+=1\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kSkSIpSFvuyP"},"source":["### Explore Captions"]},{"cell_type":"code","metadata":{"id":"2_ZbfJKGv46H"},"source":["# explore the captions\n","FIMAGES = FLICKR_PATH + '/Images'\n","print(\"Counting all .png files in: \" + FIMAGES)\n","x=0\n","for files in os.listdir(FIMAGES):\n","    if files.endswith('.jpg'):\n","        x+=1\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTtiOcFuRtAQ"},"source":["### Sample Images"]},{"cell_type":"code","metadata":{"id":"1AqGUQU4IZJ8"},"source":["# retrieve an image and caption\n","this_img = fcaps.iloc[1][0]\n","print(this_img)\n","print(fcaps.iloc[1][1])\n","image = cv2.imread(FIMAGES + '/' + this_img)\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","plt.imshow(image)\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EEKr_-xKvfa"},"source":["# retrieve a sample of images and captions\n","# Generate a random sample of index\n","image_samples = np.random.randint(0,high=len(pd.unique(fcaps['image']))-1, size=10)\n","\n","fig = plt.figure(figsize=(20,20))\n","for i,img_idx in enumerate(image_samples):\n","    axs = fig.add_subplot(5,2,i+1)\n","    axs.set_title(fcaps.iloc[img_idx][1])\n","    # Read image\n","    image = cv2.imread(FIMAGES + '/' + fcaps.iloc[img_idx][0])\n","    # convert to rgb\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    plt.imshow(image)\n","    plt.axis('off')\n","\n","plt.suptitle(\"Sample Images\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxIBVYrjRj2R"},"source":["### Data Set Metrics"]},{"cell_type":"code","metadata":{"id":"VEGBiAeHNSWA"},"source":["# Compute dataset metrics\n","# only do this for unique images (each image has multiple possible captions)\n","u_images = pd.unique(fcaps['image'])\n","u_images_df = pd.DataFrame({\n","    'image': u_images,\n","    'ind': range(0,len(u_images))\n","    })\n","u_images_df[\"ind\"] = u_images_df[\"ind\"].astype(str)\n","u_images_list = u_images_df.values.tolist()\n","compute_dataset_metrics(u_images_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnpjrzorSEyn"},"source":["## Build Data Pipelines"]},{"cell_type":"markdown","metadata":{"id":"RDAV-8ikJVSe"},"source":["### Pre-process"]},{"cell_type":"code","metadata":{"id":"DIaRJmlIJh0Y"},"source":["# Choose the top 10000 words from the vocabulary\n","top_k = 10000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n","\n","all_captions = [itm[1] for itm in fcaps_list]\n","\n","tokenizer.fit_on_texts(all_captions)\n","\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'\n","\n","# Create the tokenized vectors\n","train_seqs = tokenizer.texts_to_sequences(data_y)\n","\n","print(\"Example tokenized caption:\", train_seqs[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHNsLhgGJt5c"},"source":["# Pad each vector to the max_length of the captions\n","# If you do not provide a max_length value, pad_sequences calculates it automatically\n","cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n","\n","# Find the maximum length of any caption in the dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)\n","max_length = calc_max_length(train_seqs)\n","print(\"Max caption length:\", max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqiZbjCPJZS7"},"source":["### Split the Data"]},{"cell_type":"code","metadata":{"id":"tjDaM6kQLC6V"},"source":["# Download image files\n","IMAGE_PATH = FLICKR_PATH + '/Images'\n","\n","# Group all captions together having the same image ID.\n","image_path_to_caption = collections.defaultdict(list)\n","for val in fcaps_list:\n","  caption = f\"<start> {val[1]} <end>\"\n","  image_path = IMAGE_PATH + '/' + val[0]\n","  image_path_to_caption[image_path].append(caption)\n","\n","# randomly shuffle the image order\n","image_paths = list(image_path_to_caption.keys())\n","random.shuffle(image_paths)\n","image_paths_order = image_paths.copy()\n","\n","# create x and y data\n","data_y = []\n","data_x = []\n","for image_path in image_paths_order:\n","  caption_list = image_path_to_caption[image_path]\n","  data_y.extend(caption_list)\n","  data_x.extend([image_path] * len(caption_list))\n","\n","print(\"data_x:\",len(data_x))\n","print(\"data_y:\",len(data_y))\n","print(\"data_x:\",data_x[:5])\n","print(\"data_y:\",data_y[:5])\n","\n","# Re-group all captions together having the same image ID, for x and y data\n","img_to_cap_vector = collections.defaultdict(list)\n","for img, cap in zip(data_x, data_y):\n","  img_to_cap_vector[img].append(cap)\n","\n","# example dictionary element (keyed by image name)\n","print(\"Dictionary: \",len(img_to_cap_vector))\n","print(\"Example dictionary element: \",img_to_cap_vector[IMAGE_PATH + '/3329254388_27017bab30.jpg'])\n","\n","# train/test split\n","test_percent = 0.10\n","img_keys = list(img_to_cap_vector.keys())\n","slice_index = int(len(img_keys)*(1-test_percent))\n","img_name_train_val_keys, img_name_test_keys = img_keys[:slice_index], img_keys[slice_index:]\n","\n","train_val_x = []\n","train_val_y = []\n","for imgt in img_name_train_val_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  train_val_x.extend([imgt] * capt_len)\n","  train_val_y.extend(img_to_cap_vector[imgt])\n","\n","test_x = []\n","test_y = []\n","for imgt in img_name_test_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  test_x.extend([imgt] * capt_len)\n","  test_y.extend(img_to_cap_vector[imgt])\n","\n","# train/val split\n","val_percent = 0.20\n","img_keys = list(img_to_cap_vector.keys())\n","slice_index = int(len(img_keys)*(1-val_percent))\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","\n","train_x = []\n","train_y = []\n","for imgt in img_name_train_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  train_x.extend([imgt] * capt_len)\n","  train_y.extend(img_to_cap_vector[imgt])\n","\n","val_x = []\n","val_y = []\n","for imgt in img_name_val_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  val_x.extend([imgt] * capt_len)\n","  val_y.extend(img_to_cap_vector[imgt])\n","\n","print(\"train_x count:\",len(train_x))\n","print(\"train_y count:\",len(train_y))\n","\n","print(\"validate_x count:\",len(validate_x))\n","print(\"validate_y count:\",len(validate_y))\n","\n","print(\"test_x count:\",len(test_x))\n","print(\"test_y count:\",len(test_y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJ4R19iZTHqK"},"source":["### Create TF Datasets"]},{"cell_type":"code","metadata":{"id":"hg1yY8HprFT1"},"source":["# Feel free to change these parameters according to your system's configuration\n","# top_k = 10000\n","# max_length = 37\n","batch_size = 128\n","# BUFFER_SIZE = 1000\n","train_shuffle_buffer_size= len(train_x)\n","validation_shuffle_buffer_size= len(val_x)\n","embedding_dim = 256\n","units = 512\n","vocab_size = top_k + 1\n","num_steps = len(train_x) // batch_size\n","image_width = 224 # 255 or 299?\n","image_height = 224 # 255 or 299?\n","num_channels = 3\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","features_shape = 2048\n","attention_features_shape = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuVB4V7ure5A"},"source":["# dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","# Create TF Dataset\n","train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n","validation_data = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n","test_data = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n","\n","# Load Image\n","def load_image(path, label):\n","  image = tf.io.read_file(path)\n","  image = tf.image.decode_jpeg(image, channels=num_channels)\n","  image = tf.image.resize(image, [image_height,image_width])\n","  return image, label\n","\n","# Normalize pixels\n","def normalize(image, label):\n","  image = image/255\n","  image = tf.keras.applications.inception_v3.preprocess_input(image)\n","  #image = keras.applications.mobilenet.preprocess_input(image)\n","  return image, label\n","\n","#############\n","# Train data\n","#############\n","# Apply all data processing logic\n","train_data = train_data.shuffle(buffer_size=train_shuffle_buffer_size)\n","train_data = train_data.map(load_image, num_parallel_calls=AUTOTUNE)\n","train_data = train_data.map(normalize, num_parallel_calls=AUTOTUNE)\n","train_data = train_data.batch(batch_size)\n","train_data = train_data.prefetch(AUTOTUNE)\n","\n","##################\n","# Validation data\n","##################\n","# Apply all data processing logic\n","validation_data = validation_data.shuffle(buffer_size=validation_shuffle_buffer_size)\n","validation_data = validation_data.map(load_image, num_parallel_calls=AUTOTUNE)\n","validation_data = validation_data.map(normalize, num_parallel_calls=AUTOTUNE)\n","validation_data = validation_data.batch(batch_size)\n","validation_data = validation_data.prefetch(AUTOTUNE)\n","\n","############\n","# Test data\n","############\n","# Apply all data processing logic\n","test_data = test_data.map(load_image, num_parallel_calls=AUTOTUNE)\n","test_data = test_data.map(normalize, num_parallel_calls=AUTOTUNE)\n","test_data = test_data.batch(batch_size)\n","test_data = test_data.prefetch(AUTOTUNE)\n","\n","print(\"train_data\",train_data)\n","print(\"validation_data\",validation_data)\n","print(\"test_data\",test_data)"],"execution_count":null,"outputs":[]}]}